{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.12.3\n",
      "2.31.0\n"
     ]
    }
   ],
   "source": [
    "import bs4\n",
    "\n",
    "print(bs4.__version__)\n",
    "print(requests.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Empezamos el scraping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Obtener el HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "URL_BASE = 'https://scrapepark.org/spanish/'\n",
    "pedido_obtenido = requests.get(URL_BASE)\n",
    "html_obtenido = pedido_obtenido.text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Parsear el HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(html_obtenido,\"html.parser\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## El método find() nos permite quedarnos con la información asociada a una etiqueta de HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "primer_h2 = soup.find('h2')\n",
    "print(primer_h2)\n",
    "print(primer_h2.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## El método find_all() busca todos los elementos de la página con esa etiqueta y devuelve una \"lista\" que los contiene (en realidad devuelve un objeto de la clase bs4.element.ResultSet)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h2_todos=soup.find_all('h2')\n",
    "print(h2_todos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Si usamos el parametro limit=1, emulamos al método find"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h2_uno_solo = soup.find_all('h2',limit=1)\n",
    "print(h2_uno_solo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Podemos iterar sobre el objeto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h2=soup.find_all('h2')\n",
    "for i in h2:\n",
    "    print(i.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## get_text() para más funcionalidades"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for seccion in h2:\n",
    "    print(seccion.get_text(strip=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utilizando atributos de las etiquetas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<div class=\"heading-container heading-center\" id=\"about\">\n",
      "<h2>¿Por qué comprar con nosotros?</h2>\n",
      "</div>\n",
      " \n",
      "<div class=\"heading-container heading-center\" id=\"products\">\n",
      "<h2>Nuestros productos</h2>\n",
      "</div>\n",
      " \n",
      "<div class=\"heading-container heading-center\">\n",
      "<h3>Suscríbete para obtener descuentos y ofertas</h3>\n",
      "</div>\n",
      " \n",
      "<div class=\"heading-container heading-center\">\n",
      "<h2>Testimonios de clientes</h2>\n",
      "</div>\n",
      " \n"
     ]
    }
   ],
   "source": [
    "divs = soup.find_all('div', class_ = 'heading-container heading-center')\n",
    "for div in divs:\n",
    "    print(div)\n",
    "    print(' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Todas las etiquetas que tengan el atributo \"src\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ScrapePark.org Logo\n",
      "Parque de patinaje\n",
      "Variedad de patinetas en nuestra tienda\n",
      "Patineta\n",
      "Patineta\n",
      "Patineta\n",
      "Patineta\n",
      "Patineta\n",
      "Patineta\n",
      "Patineta\n",
      "Patineta\n",
      "Patineta\n",
      "Patineta\n",
      "Patineta\n",
      "Patineta\n",
      "Cliente\n",
      "Cliente\n",
      "Cliente\n",
      "Logo de ScrapePark.org\n",
      "Logo de freeCodeCamp\n"
     ]
    }
   ],
   "source": [
    "src_todos = soup.find_all('img')\n",
    "for elemento in src_todos:\n",
    "    print(elemento['alt'])\n",
    "    '''if elemento['src'].endswith('.jpg'):\n",
    "        print(elemento)'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bajar todas las imagenes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,imagen in enumerate(src_todos):\n",
    "    if imagen['src'].endswith('png'):\n",
    "        print(imagen['src'])\n",
    "        r=requests.get(f'https://scrapepark.org/spanish/{imagen['src']}')\n",
    "        with open(f'imagen_{i}.png','wb') as f:\n",
    "            f.write(r.content)            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Información de tablas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Longboard', '$80', '$85', '$90', '$62', '$150']\n"
     ]
    }
   ],
   "source": [
    "URL_BASE = 'https://scrapepark.org/spanish'\n",
    "URL_TABLA = soup.find_all('iframe')[0]['src']\n",
    "\n",
    "request_tabla = requests.get(f'{URL_BASE}/{URL_TABLA}')\n",
    "\n",
    "html_tabla = request_tabla.text\n",
    "soup_tabla = BeautifulSoup(html_tabla,'html.parser')\n",
    "soup_tabla.find('table')\n",
    "\n",
    "productos_faltantes = soup_tabla.find_all(['th','td'], attrs={'style':'color: red;'})\n",
    "productos_fal = [talle.text for talle in productos_faltantes ]\n",
    "\n",
    "print(productos_fal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "divs = soup.find_all('div', class_='detail-box')\n",
    "productos = []\n",
    "precios = []\n",
    "\n",
    "for div in divs:\n",
    "    if div.h6 is not None and 'Patineta' in div.h5.text:\n",
    "        producto = div.h5.get_text(strip=True)\n",
    "        precio = div.h6.get_text(strip=True).replace('$','')\n",
    "        print(f'producto: {producto:<16} | precio: {precio}')\n",
    "        productos.append(producto)\n",
    "        precios.append(precio)\n",
    "print(*productos)\n",
    "print(*precios)        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cambios que dependen de la URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://scrapepark.org/spanish/contact1\n",
      "Texto que cambia entre páginas en contacto 1 :)\n",
      "https://scrapepark.org/spanish/contact2\n",
      "Texto que cambia entre páginas en contacto 2 :)\n"
     ]
    }
   ],
   "source": [
    "URL_BASE = 'https://scrapepark.org/spanish/contact'\n",
    "\n",
    "for i in range(1,3):\n",
    "    URL_FINAL = f'{URL_BASE}{i}'\n",
    "    print(URL_FINAL)\n",
    "    r = requests.get(URL_FINAL)\n",
    "    soup = BeautifulSoup(r.text,'html.parser')\n",
    "    print(soup.h5.text) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Datos que no sabemos en que parte de la página se encuentran"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:11: SyntaxWarning: invalid escape sequence '\\d'\n",
      "<>:11: SyntaxWarning: invalid escape sequence '\\d'\n",
      "C:\\Users\\jamir\\AppData\\Local\\Temp\\ipykernel_14364\\229612336.py:11: SyntaxWarning: invalid escape sequence '\\d'\n",
      "  telefonos = soup.find_all(string=re.compile('\\d+-\\d+-\\d+'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[': 4-444-4444']\n"
     ]
    }
   ],
   "source": [
    "# Expresiones regulares\n",
    "import re\n",
    "\n",
    "# 1. Obtener el HTML\n",
    "URL_BASE = 'https://scrapepark.org/spanish'\n",
    "pedido_obtenido = requests.get(URL_BASE)\n",
    "html_obtenido = pedido_obtenido.text\n",
    "\n",
    "# 2.'Parsear' ese HTML\n",
    "soup = BeautifulSoup(html_obtenido,'html.parser')\n",
    "telefonos = soup.find_all(string=re.compile('\\d+-\\d+-\\d+'))\n",
    "print(telefonos)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
